# Extra reads	
## ML advance topics
| Paper | Why |
| ------------- | ------------- |
| [Sampling techniques](https://towardsdatascience.com/sampling-techniques-a4e34111d808) | Stratifield sampling is popular. |
| [A Comparative Study of Efficient Initialization Methods for the k-means Clustering Algorithm](https://arxiv.org/pdf/1209.1960.pdf) and [kmeans initialization, Coursera](https://www.coursera.org/lecture/cluster-analysis/3-3-initialization-of-k-means-clustering-bPyBl)| Fundamentals about kemans, favourite topics in interviews i.e: LinkedIn|
| [A Comprehensive Survey of Clustering Algorithms](https://link.springer.com/content/pdf/10.1007/s40745-015-0040-1.pdf) | Clustering fundamentals |
| [A Tutorial on Spectral Clustering](https://arxiv.org/pdf/0711.0189.pdf)| Sepctral clustering is intuitive and quite popular. |
| [Partial residual plot](https://en.wikipedia.org/wiki/Partial_residual_plot) | Useful for model diagnosis. |
| [Compare GINI index and Information Gain](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf) | Intuition behind Decision Tree, RandomForest |
| [Explain tf-idf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.97.7340&rep=rep1&type=pdf) | Fundamentals about tf-idf. |
| [Understanding L-BFGS](https://aria42.com/blog/2014/12/understanding-lbfgs) | Advance about optimization, rarely asked in interview |
| [Optimizer Quasi newton method](http://www.seas.ucla.edu/~vandenbe/236C/lectures/qnewton.pdf) | Advance about optimization. |


## DL classic papers
| Paper | Why |
| ------------- | ------------- |
| [Understanding the Difficulty of Training Deep Feedforward Neural Networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) | Classic paper (2010) about initialization, sigmoid etc |
| [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf) | Classic paper (2015) about ReLU, PReLU|
| [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf%20http://arxiv.org/abs/1502.03167.pdf) | Classic paper about BatchNorm |
| [Dropout - A Simple Way to Prevent Neural Networks from Overfitting](https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) | Classic paper about Dropout |
| [Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) | Classic ResNet |
| [On Large-Batch Training for Deep Learning - Generalization Gap and Sharp Minima](https://arxiv.org/pdf/1609.04836.pdf,) | Practical technique for large batch training |


## DL advance topics
| Paper | Why |
| ------------- | ------------- |
| [Calibration in modern neural network](https://arxiv.org/pdf/1706.04599.pdf) | Important topics in ML system design i.e: facebook |
| [Attention model](https://www.youtube.com/watch?v=quoGRI-1l0A&list=RDCMUCcIXc5mJsHVYTZR1maL5l9w&index=2) | Fundamentals in Attention, powerful architecture in NLP |
| [Ilya's thesis](https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf) | Network in network |


## NLP
| Paper | Why |
| ------------- | ------------- |
| [Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester2_2009_10/4443871.pdf) | Classic paper (2008) in NLP |
| [Natural Language Processing (Almost) from Scratch](https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf) | Classic paper (2011) in NLP |
| [Word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) | The classic paper in NLP, still popular in industry: Uber, DoorDash, Twitter etc |
| [GloVe - Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162.pdf) | Classic paper (2014) in NLP |
| [Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf) | Cool tricks in NLP tasks |
| [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) | The famous BERT |
| [Smart Reply - Automated Response Suggestion for Email](https://arxiv.org/pdf/1606.04870.pdf) | NLP application, useful for ML system design |
| [Enriching Word Vectors with Subword Information](https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00051) | Simple and fast method to train NLP task in Facebook|
| [Neural Approaches to Conversational AI](https://arxiv.org/pdf/1809.08267.pdf) | Comprehensive survey (2018) about chatbots|

## Recent breakthrough
- [Feb 2021, recent breakthrough ClubHouse chat](https://victordibia.com/blog/ai-breakthroughs-feb21/)

# Notes
* If you're interested to learn more about paid ML system design course on educative.io with more examples, [click here](https://rebrand.ly/mlsd_launch).
* If you find this helpful, you can Sponsor this project. It's cool if you don't. 
